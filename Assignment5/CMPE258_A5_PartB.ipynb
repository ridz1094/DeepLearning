{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CMPE-258-A5-PartB.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"J9XeXakTLkZ5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619341120560,"user_tz":420,"elapsed":4040,"user":{"displayName":"Riddhi Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggz1Tm3h5UIJzG3vpJXI5xI7vVZY0idHw_n7Lf7=s64","userId":"07907048644654273054"}},"outputId":"b0ec74bc-bb44-4e00-ad12-4bca4f37b17a"},"source":["!pip install tfx"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tfx in /usr/local/lib/python3.7/dist-packages (0.29.0)\n","Requirement already satisfied: click<8,>=7 in /usr/local/lib/python3.7/dist-packages (from tfx) (7.1.2)\n","Requirement already satisfied: absl-py<0.13,>=0.9 in /usr/local/lib/python3.7/dist-packages (from tfx) (0.12.0)\n","Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /usr/local/lib/python3.7/dist-packages (from tfx) (1.12.8)\n","Requirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tfx) (3.12.4)\n","Requirement already satisfied: tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2 in /usr/local/lib/python3.7/dist-packages (from tfx) (2.4.1)\n","Requirement already satisfied: ml-metadata<0.30,>=0.29 in /usr/local/lib/python3.7/dist-packages (from tfx) (0.29.0)\n","Requirement already satisfied: apache-beam[gcp]<3,>=2.28 in /usr/local/lib/python3.7/dist-packages (from tfx) (2.28.0)\n","Requirement already satisfied: tfx-bsl<0.30,>=0.29 in /usr/local/lib/python3.7/dist-packages (from tfx) (0.29.0)\n","Requirement already satisfied: pyyaml<6,>=3.12 in /usr/local/lib/python3.7/dist-packages (from tfx) (3.13)\n","Requirement already satisfied: keras-tuner<1.0.2,>=1 in /usr/local/lib/python3.7/dist-packages (from tfx) (1.0.1)\n","Requirement already satisfied: attrs<21,>=19.3.0 in /usr/local/lib/python3.7/dist-packages (from tfx) (20.3.0)\n","Requirement already satisfied: kubernetes<12,>=10.0.1 in /usr/local/lib/python3.7/dist-packages (from tfx) (11.0.0)\n","Requirement already satisfied: tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15 in /usr/local/lib/python3.7/dist-packages (from tfx) (2.4.1)\n","Requirement already satisfied: docker<5,>=4.1 in /usr/local/lib/python3.7/dist-packages (from tfx) (4.4.4)\n","Requirement already satisfied: tensorflow-model-analysis<0.30,>=0.29 in /usr/local/lib/python3.7/dist-packages (from tfx) (0.29.0)\n","Requirement already satisfied: tensorflow-cloud<0.2,>=0.1 in /usr/local/lib/python3.7/dist-packages (from tfx) (0.1.13)\n","Requirement already satisfied: jinja2<3,>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from tfx) (2.11.3)\n","Requirement already satisfied: tensorflow-data-validation<0.30,>=0.29 in /usr/local/lib/python3.7/dist-packages (from tfx) (0.29.0)\n","Requirement already satisfied: numpy<1.20,>=1.16 in /usr/local/lib/python3.7/dist-packages (from tfx) (1.19.5)\n","Requirement already satisfied: ml-pipelines-sdk==0.29.0 in /usr/local/lib/python3.7/dist-packages (from tfx) (0.29.0)\n","Requirement already satisfied: six<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from tfx) (1.15.0)\n","Requirement already satisfied: packaging<21,>=20 in /usr/local/lib/python3.7/dist-packages (from tfx) (20.9)\n","Requirement already satisfied: grpcio<2,>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from tfx) (1.32.0)\n","Requirement already satisfied: pyarrow<3,>=1 in /usr/local/lib/python3.7/dist-packages (from tfx) (2.0.0)\n","Requirement already satisfied: tensorflow-transform<0.30,>=0.29 in /usr/local/lib/python3.7/dist-packages (from tfx) (0.29.0)\n","Requirement already satisfied: tensorflow-hub<0.10,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from tfx) (0.9.0)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (3.0.1)\n","Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (1.28.1)\n","Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (1.26.3)\n","Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (0.17.4)\n","Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (0.0.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf<4,>=3.12.2->tfx) (56.0.0)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tfx) (1.6.3)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tfx) (3.3.0)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tfx) (0.3.3)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tfx) (1.1.0)\n","Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tfx) (2.4.1)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tfx) (3.7.4.3)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tfx) (1.12.1)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tfx) (1.1.2)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tfx) (0.36.2)\n","Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tfx) (2.10.0)\n","Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tfx) (2.4.0)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tfx) (0.2.0)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tfx) (1.12)\n","Requirement already satisfied: future<1.0.0,>=0.18.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (0.18.2)\n","Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (2.25.1)\n","Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (3.11.3)\n","Requirement already satisfied: avro-python3!=1.9.2,<1.10.0,>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (1.9.2.1)\n","Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (2018.9)\n","Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (1.3.0)\n","Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (1.7)\n","Requirement already satisfied: oauth2client<5,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (4.1.3)\n","Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (2.8.1)\n","Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (2.6.0)\n","Requirement already satisfied: mock<3.0.0,>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (2.0.0)\n","Requirement already satisfied: fastavro<2,>=0.21.4 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (1.4.0)\n","Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (0.3.1.1)\n","Requirement already satisfied: google-cloud-videointelligence<2,>=1.8.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (1.16.1)\n","Requirement already satisfied: cachetools<5,>=3.1.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (4.2.1)\n","Requirement already satisfied: google-cloud-pubsub<2,>=0.39.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (1.7.0)\n","Requirement already satisfied: google-cloud-spanner<2,>=1.13.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (1.19.1)\n","Requirement already satisfied: google-cloud-core<2,>=0.28.1; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (1.0.3)\n","Requirement already satisfied: grpcio-gcp<1,>=0.2.2; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (0.2.2)\n","Requirement already satisfied: google-apitools<0.5.32,>=0.5.31; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (0.5.31)\n","Requirement already satisfied: google-cloud-dlp<2,>=0.12.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (1.0.0)\n","Requirement already satisfied: google-cloud-vision<2,>=0.38.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (1.0.0)\n","Requirement already satisfied: google-cloud-datastore<2,>=1.7.1; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (1.8.0)\n","Requirement already satisfied: google-cloud-bigtable<2,>=0.31.1; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (1.7.0)\n","Requirement already satisfied: google-cloud-build<3,>=2.0.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (2.0.0)\n","Requirement already satisfied: google-cloud-bigquery<2,>=1.6.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (1.21.0)\n","Requirement already satisfied: google-cloud-language<2,>=1.3.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tfx) (1.3.0)\n","Requirement already satisfied: pandas<2,>=1.0 in /usr/local/lib/python3.7/dist-packages (from tfx-bsl<0.30,>=0.29->tfx) (1.1.5)\n","Requirement already satisfied: tensorflow-metadata<0.30,>=0.29 in /usr/local/lib/python3.7/dist-packages (from tfx-bsl<0.30,>=0.29->tfx) (0.29.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from keras-tuner<1.0.2,>=1->tfx) (4.41.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner<1.0.2,>=1->tfx) (1.4.1)\n","Requirement already satisfied: terminaltables in /usr/local/lib/python3.7/dist-packages (from keras-tuner<1.0.2,>=1->tfx) (3.1.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from keras-tuner<1.0.2,>=1->tfx) (0.22.2.post1)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from keras-tuner<1.0.2,>=1->tfx) (0.8.9)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from keras-tuner<1.0.2,>=1->tfx) (0.4.4)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from kubernetes<12,>=10.0.1->tfx) (0.58.0)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.7/dist-packages (from kubernetes<12,>=10.0.1->tfx) (1.3.0)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.7/dist-packages (from kubernetes<12,>=10.0.1->tfx) (1.24.3)\n","Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.7/dist-packages (from kubernetes<12,>=10.0.1->tfx) (2020.12.5)\n","Requirement already satisfied: ipython<8,>=7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-analysis<0.30,>=0.29->tfx) (7.22.0)\n","Requirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-analysis<0.30,>=0.29->tfx) (7.6.3)\n","Requirement already satisfied: tensorflow-datasets<3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-cloud<0.2,>=0.1->tfx) (3.0.0)\n","Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.7/dist-packages (from tensorflow-cloud<0.2,>=0.1->tfx) (1.18.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<3,>=2.7.3->tfx) (1.1.1)\n","Requirement already satisfied: joblib<0.15,>=0.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow-data-validation<0.30,>=0.29->tfx) (0.14.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging<21,>=20->tfx) (2.4.7)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client<2,>=1.7.8->tfx) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client<2,>=1.7.8->tfx) (4.7.2)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->tfx) (1.53.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tfx) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tfx) (3.3.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tfx) (0.4.4)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tfx) (1.8.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.28->tfx) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.28->tfx) (3.0.4)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]<3,>=2.28->tfx) (0.4.8)\n","Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.28->tfx) (0.6.2)\n","Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.7/dist-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]<3,>=2.28->tfx) (5.5.1)\n","Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-pubsub<2,>=0.39.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.28->tfx) (0.12.3)\n","Requirement already satisfied: fasteners>=0.14 in /usr/local/lib/python3.7/dist-packages (from google-apitools<0.5.32,>=0.5.31; extra == \"gcp\"->apache-beam[gcp]<3,>=2.28->tfx) (0.16)\n","Requirement already satisfied: proto-plus>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-build<3,>=2.0.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.28->tfx) (1.18.1)\n","Requirement already satisfied: libcst>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from google-cloud-build<3,>=2.0.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.28->tfx) (0.3.18)\n","Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<2,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.28->tfx) (0.4.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib->kubernetes<12,>=10.0.1->tfx) (3.1.0)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (0.18.0)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (3.0.18)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (0.7.5)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (2.6.1)\n","Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (4.8.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (0.2.0)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (5.0.5)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (3.5.1)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (1.0.0)\n","Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (4.10.1)\n","Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (5.1.3)\n","Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets<3.1.0->tensorflow-cloud<0.2,>=0.1->tfx) (2.3)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tfx) (3.10.1)\n","Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from libcst>=0.2.5->google-cloud-build<3,>=2.0.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.28->tfx) (0.6.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (0.8.2)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (0.2.5)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (0.7.0)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (0.2.0)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (5.3.1)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (5.3.5)\n","Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (5.1.1)\n","Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (2.6.0)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (4.7.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tfx) (3.4.1)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from typing-inspect>=0.4.0->libcst>=0.2.5->google-cloud-build<3,>=2.0.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.28->tfx) (0.4.3)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (0.9.4)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (5.6.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (1.5.0)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (22.0.3)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (0.7.1)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (3.3.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (1.4.3)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (0.4.4)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (0.8.4)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (0.3)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.30,>=0.29->tfx) (0.5.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YIqpWK9efviJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619341124455,"user_tz":420,"elapsed":7929,"user":{"displayName":"Riddhi Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggz1Tm3h5UIJzG3vpJXI5xI7vVZY0idHw_n7Lf7=s64","userId":"07907048644654273054"}},"outputId":"b5092a35-c538-47a6-843c-34c09f2811f3"},"source":["import os\n","import pprint\n","import numpy as np\n","import tempfile\n","import urllib\n","\n","import absl\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_model_analysis as tfma\n","tf.get_logger().propagate = False\n","pp = pprint.PrettyPrinter()\n","\n","import tfx\n","from tfx.components import CsvExampleGen\n","from typing import Dict, List, Text\n","from tfx.components import Evaluator\n","from tfx.components import ExampleValidator\n","from tfx.components import Pusher\n","from tfx.components import ResolverNode\n","from tfx.components import SchemaGen\n","from tfx.components import StatisticsGen\n","from tfx.components import Trainer\n","from tfx.components import Transform\n","from tfx.components.base import executor_spec\n","from tfx.components.trainer.executor import GenericExecutor\n","from tfx.dsl.experimental import latest_blessed_model_resolver\n","from tfx.orchestration import metadata\n","from tfx.orchestration import pipeline\n","from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n","from tfx.proto import pusher_pb2\n","from tfx.proto import trainer_pb2\n","from tfx.types import Channel\n","from tfx.types.standard_artifacts import Model\n","from tfx.types.standard_artifacts import ModelBlessing\n","from tfx.utils.dsl_utils import external_input\n","\n","\n","%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"wCZTHRy0N1D6"},"source":["Let's check the library versions."]},{"cell_type":"code","metadata":{"id":"eZ4K18_DN2D8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619341124456,"user_tz":420,"elapsed":7925,"user":{"displayName":"Riddhi Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggz1Tm3h5UIJzG3vpJXI5xI7vVZY0idHw_n7Lf7=s64","userId":"07907048644654273054"}},"outputId":"96934158-301f-4d7a-b1e2-5a0fcbf290a8"},"source":["print('TensorFlow version: {}'.format(tf.__version__))\n","print('TFX version: {}'.format(tfx.__version__))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow version: 2.4.1\n","TFX version: 0.29.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"grFKIgyXOAVM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619341124811,"user_tz":420,"elapsed":8277,"user":{"displayName":"Riddhi Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggz1Tm3h5UIJzG3vpJXI5xI7vVZY0idHw_n7Lf7=s64","userId":"07907048644654273054"}},"outputId":"61efff1a-0756-453e-9d5c-28c285546546"},"source":["!rm -rf data.*\n","!rm -rf *trainer.py\n","!sudo rm -r /content/tfx"],"execution_count":null,"outputs":[{"output_type":"stream","text":["rm: cannot remove '/content/tfx': No such file or directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r_17y324BB0K"},"source":["! cd /content/\n","! mkdir /content/tfx/\n","! mkdir /content/tfx/pipelines\n","! mkdir /content/tfx/metadata\n","! mkdir /content/tfx/logs\n","! mkdir /content/tfx/data\n","! mkdir /content/tfx/serving_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n2cMMAbSkGfX"},"source":["### Download example data\n","We download the example dataset for use in our TFX pipeline.\n","\n","The dataset we're using is the [Taxi Trips dataset](https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew) released by the City of Chicago. The columns in this dataset are:\n","\n","<table>\n","<tr><td>pickup_community_area</td><td>fare</td><td>trip_start_month</td></tr>\n","<tr><td>trip_start_hour</td><td>trip_start_day</td><td>trip_start_timestamp</td></tr>\n","<tr><td>pickup_latitude</td><td>pickup_longitude</td><td>dropoff_latitude</td></tr>\n","<tr><td>dropoff_longitude</td><td>trip_miles</td><td>pickup_census_tract</td></tr>\n","<tr><td>dropoff_census_tract</td><td>payment_type</td><td>company</td></tr>\n","<tr><td>trip_seconds</td><td>dropoff_community_area</td><td>tips</td></tr>\n","</table>\n","\n","With this dataset, we will build a model that predicts the `fare` of a trip."]},{"cell_type":"code","metadata":{"id":"BywX6OUEhAqn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619341126232,"user_tz":420,"elapsed":9693,"user":{"displayName":"Riddhi Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggz1Tm3h5UIJzG3vpJXI5xI7vVZY0idHw_n7Lf7=s64","userId":"07907048644654273054"}},"outputId":"884fdd40-3e42-4bec-8249-bb0e464805f5"},"source":["!wget https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-04-25 08:58:45--  https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1922812 (1.8M) [text/plain]\n","Saving to: ‘data.csv’\n","\n","data.csv            100%[===================>]   1.83M  8.66MB/s    in 0.2s    \n","\n","2021-04-25 08:58:46 (8.66 MB/s) - ‘data.csv’ saved [1922812/1922812]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SakyMaydJ4G5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619341126505,"user_tz":420,"elapsed":9961,"user":{"displayName":"Riddhi Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggz1Tm3h5UIJzG3vpJXI5xI7vVZY0idHw_n7Lf7=s64","userId":"07907048644654273054"}},"outputId":"a2c33b1f-bcd5-4481-c518-8228f0839319"},"source":["df = pd.read_csv('/content/data.csv')\n","\n","##Drop useless columns\n","df = df.drop(['trip_start_timestamp','trip_miles','pickup_census_tract',\n","              'dropoff_census_tract','trip_seconds','payment_type','tips', \n","              'company','dropoff_community_area','pickup_community_area'], axis=1)\n","\n","#Drop NA rows\n","df = df.dropna()\n","\n","##Keep a test set for final testing( TFX internally splits train and validation data )\n","np.random.seed(seed=2)\n","msk = np.random.rand(len(df)) < 0.9\n","traindf = df[msk]\n","evaldf = df[~msk]\n","\n","print(len(traindf))\n","print(len(evaldf))\n","\n","traindf.to_csv(\"/content/tfx/data/data_trans.csv\", index=False, header=True)\n","evaldf.to_csv(\"eval.csv\", index=False, header=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["13077\n","1442\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"blZC1sIQOWfH"},"source":["Take a quick look at the CSV file."]},{"cell_type":"code","metadata":{"id":"c5YPeLPFOXaD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619341126698,"user_tz":420,"elapsed":10150,"user":{"displayName":"Riddhi Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggz1Tm3h5UIJzG3vpJXI5xI7vVZY0idHw_n7Lf7=s64","userId":"07907048644654273054"}},"outputId":"cab3abd9-cb54-4fba-c553-48da19addc89"},"source":["!head {_data_filepath}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["head: cannot open '{_data_filepath}' for reading: No such file or directory\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ufJKQ6OvkJlY"},"source":["### Set up pipeline paths"]},{"cell_type":"code","metadata":{"id":"RsH0ckYiADx5"},"source":["##Define all constant\n","_tfx_root = os.path.join(os.getcwd(), 'tfx');        # Create location ~/tfx\n","_pipeline_root = os.path.join(_tfx_root, 'pipelines');      # Join ~/tfx/pipelines/\n","_metadata_db_root = os.path.join(_tfx_root, 'metadata.db');    # Join ~/tfx/metadata.db\n","_log_root = os.path.join(_tfx_root, 'logs');\n","_model_root = os.path.join(_tfx_root, 'model');\n","_data_root = os.path.join(_tfx_root, 'data');\n","_serving_model_dir = os.path.join(_tfx_root, 'serving_model')\n","_data_filepath = os.path.join(_data_root, \"data_trans.csv\")\n","\n","_input_fn_module_file = 'inputfn_trainer.py'\n","_constants_module_file = 'constants_trainer.py'\n","_model_trainer_module_file = 'model_trainer.py'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ONIE_hdkPS4"},"source":["### Create the InteractiveContext\n","Last, we create an InteractiveContext, which will allow us to run TFX components interactively in this notebook."]},{"cell_type":"code","metadata":{"id":"0Rh6K5sUf9dd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619341126699,"user_tz":420,"elapsed":10145,"user":{"displayName":"Riddhi Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggz1Tm3h5UIJzG3vpJXI5xI7vVZY0idHw_n7Lf7=s64","userId":"07907048644654273054"}},"outputId":"c8a462a7-6173-4844-d9c6-b613f79a6317"},"source":["# Here, we create an InteractiveContext using default parameters. This will\n","# use a temporary directory with an ephemeral ML Metadata database instance.\n","# To use your own pipeline root or database, the optional properties\n","# `pipeline_root` and `metadata_connection_config` may be passed to\n","# InteractiveContext. Calls to InteractiveContext are no-ops outside of the\n","# notebook.\n","context = InteractiveContext(pipeline_root=_tfx_root)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:absl:InteractiveContext metadata_connection_config not provided: using SQLite ML Metadata database at /content/tfx/metadata.sqlite.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"HdQWxfsVkzdJ"},"source":["## Run TFX components interactively\n","In the cells that follow, we create TFX components one-by-one, run each of them, and visualize their output artifacts."]},{"cell_type":"markdown","metadata":{"id":"L9fwt9gQk3BR"},"source":["### ExampleGen\n"]},{"cell_type":"code","metadata":{"id":"PyXjuMt8f-9u","colab":{"base_uri":"https://localhost:8080/","height":281},"executionInfo":{"status":"ok","timestamp":1619341131859,"user_tz":420,"elapsed":15303,"user":{"displayName":"Riddhi Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggz1Tm3h5UIJzG3vpJXI5xI7vVZY0idHw_n7Lf7=s64","userId":"07907048644654273054"}},"outputId":"805a9baa-dab0-4ae5-c7d5-53b31a9e9769"},"source":["example_gen = CsvExampleGen(input=external_input(_data_root))\n","context.run(example_gen)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:absl:From <ipython-input-11-2e0190c2dd16>:1: external_input (from tfx.utils.dsl_utils) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","external_input is deprecated, directly pass the uri to ExampleGen.\n","WARNING:absl:The \"input\" argument to the CsvExampleGen component has been deprecated by \"input_base\". Please update your usage as support for this argument will be removed soon.\n","WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/javascript":["\n","        if (typeof window.interactive_beam_jquery == 'undefined') {\n","          var jqueryScript = document.createElement('script');\n","          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n","          jqueryScript.type = 'text/javascript';\n","          jqueryScript.onload = function() {\n","            var datatableScript = document.createElement('script');\n","            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n","            datatableScript.type = 'text/javascript';\n","            datatableScript.onload = function() {\n","              window.interactive_beam_jquery = jQuery.noConflict(true);\n","              window.interactive_beam_jquery(document).ready(function($){\n","                \n","              });\n","            }\n","            document.head.appendChild(datatableScript);\n","          };\n","          document.head.appendChild(jqueryScript);\n","        } else {\n","          window.interactive_beam_jquery(document).ready(function($){\n","            \n","          });\n","        }"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<style>\n",".tfx-object.expanded {\n","  padding: 4px 8px 4px 8px;\n","  background: white;\n","  border: 1px solid #bbbbbb;\n","  box-shadow: 4px 4px 2px rgba(0,0,0,0.05);\n","}\n",".tfx-object, .tfx-object * {\n","  font-size: 11pt;\n","}\n",".tfx-object > .title {\n","  cursor: pointer;\n","}\n",".tfx-object .expansion-marker {\n","  color: #999999;\n","}\n",".tfx-object.expanded > .title > .expansion-marker:before {\n","  content: '▼';\n","}\n",".tfx-object.collapsed > .title > .expansion-marker:before {\n","  content: '▶';\n","}\n",".tfx-object .class-name {\n","  font-weight: bold;\n","}\n",".tfx-object .deemphasize {\n","  opacity: 0.5;\n","}\n",".tfx-object.collapsed > table.attr-table {\n","  display: none;\n","}\n",".tfx-object.expanded > table.attr-table {\n","  display: block;\n","}\n",".tfx-object table.attr-table {\n","  border: 2px solid white;\n","  margin-top: 5px;\n","}\n",".tfx-object table.attr-table td.attr-name {\n","  vertical-align: top;\n","  font-weight: bold;\n","}\n",".tfx-object table.attr-table td.attrvalue {\n","  text-align: left;\n","}\n","</style>\n","<script>\n","function toggleTfxObject(element) {\n","  var objElement = element.parentElement;\n","  if (objElement.classList.contains('collapsed')) {\n","    objElement.classList.remove('collapsed');\n","    objElement.classList.add('expanded');\n","  } else {\n","    objElement.classList.add('collapsed');\n","    objElement.classList.remove('expanded');\n","  }\n","}\n","</script>\n","<div class=\"tfx-object expanded\"><div class = \"title\" onclick=\"toggleTfxObject(this)\"><span class=\"expansion-marker\"></span><span class=\"class-name\">ExecutionResult</span><span class=\"deemphasize\"> at 0x7fc955f24cd0</span></div><table class=\"attr-table\"><tr><td class=\"attr-name\">.execution_id</td><td class = \"attrvalue\">1</td></tr><tr><td class=\"attr-name\">.component</td><td class = \"attrvalue\"><style>\n",".tfx-object.expanded {\n","  padding: 4px 8px 4px 8px;\n","  background: white;\n","  border: 1px solid #bbbbbb;\n","  box-shadow: 4px 4px 2px rgba(0,0,0,0.05);\n","}\n",".tfx-object, .tfx-object * {\n","  font-size: 11pt;\n","}\n",".tfx-object > .title {\n","  cursor: pointer;\n","}\n",".tfx-object .expansion-marker {\n","  color: #999999;\n","}\n",".tfx-object.expanded > .title > .expansion-marker:before {\n","  content: '▼';\n","}\n",".tfx-object.collapsed > .title > .expansion-marker:before {\n","  content: '▶';\n","}\n",".tfx-object .class-name {\n","  font-weight: bold;\n","}\n",".tfx-object .deemphasize {\n","  opacity: 0.5;\n","}\n",".tfx-object.collapsed > table.attr-table {\n","  display: none;\n","}\n",".tfx-object.expanded > table.attr-table {\n","  display: block;\n","}\n",".tfx-object table.attr-table {\n","  border: 2px solid white;\n","  margin-top: 5px;\n","}\n",".tfx-object table.attr-table td.attr-name {\n","  vertical-align: top;\n","  font-weight: bold;\n","}\n",".tfx-object table.attr-table td.attrvalue {\n","  text-align: left;\n","}\n","</style>\n","<script>\n","function toggleTfxObject(element) {\n","  var objElement = element.parentElement;\n","  if (objElement.classList.contains('collapsed')) {\n","    objElement.classList.remove('collapsed');\n","    objElement.classList.add('expanded');\n","  } else {\n","    objElement.classList.add('collapsed');\n","    objElement.classList.remove('expanded');\n","  }\n","}\n","</script>\n","<div class=\"tfx-object collapsed\"><div class = \"title\" onclick=\"toggleTfxObject(this)\"><span class=\"expansion-marker\"></span><span class=\"class-name\">CsvExampleGen</span><span class=\"deemphasize\"> at 0x7fc952059910</span></div><table class=\"attr-table\"><tr><td class=\"attr-name\">.inputs</td><td class = \"attrvalue\">{}</td></tr><tr><td class=\"attr-name\">.outputs</td><td class = \"attrvalue\"><table class=\"attr-table\"><tr><td class=\"attr-name\">['examples']</td><td class = \"attrvalue\"><style>\n",".tfx-object.expanded {\n","  padding: 4px 8px 4px 8px;\n","  background: white;\n","  border: 1px solid #bbbbbb;\n","  box-shadow: 4px 4px 2px rgba(0,0,0,0.05);\n","}\n",".tfx-object, .tfx-object * {\n","  font-size: 11pt;\n","}\n",".tfx-object > .title {\n","  cursor: pointer;\n","}\n",".tfx-object .expansion-marker {\n","  color: #999999;\n","}\n",".tfx-object.expanded > .title > .expansion-marker:before {\n","  content: '▼';\n","}\n",".tfx-object.collapsed > .title > .expansion-marker:before {\n","  content: '▶';\n","}\n",".tfx-object .class-name {\n","  font-weight: bold;\n","}\n",".tfx-object .deemphasize {\n","  opacity: 0.5;\n","}\n",".tfx-object.collapsed > table.attr-table {\n","  display: none;\n","}\n",".tfx-object.expanded > table.attr-table {\n","  display: block;\n","}\n",".tfx-object table.attr-table {\n","  border: 2px solid white;\n","  margin-top: 5px;\n","}\n",".tfx-object table.attr-table td.attr-name {\n","  vertical-align: top;\n","  font-weight: bold;\n","}\n",".tfx-object table.attr-table td.attrvalue {\n","  text-align: left;\n","}\n","</style>\n","<script>\n","function toggleTfxObject(element) {\n","  var objElement = element.parentElement;\n","  if (objElement.classList.contains('collapsed')) {\n","    objElement.classList.remove('collapsed');\n","    objElement.classList.add('expanded');\n","  } else {\n","    objElement.classList.add('collapsed');\n","    objElement.classList.remove('expanded');\n","  }\n","}\n","</script>\n","<div class=\"tfx-object collapsed\"><div class = \"title\" onclick=\"toggleTfxObject(this)\"><span class=\"expansion-marker\"></span><span class=\"class-name\">Channel</span> of type <span class=\"class-name\">'Examples'</span> (1 artifact)<span class=\"deemphasize\"> at 0x7fc951fb5a10</span></div><table class=\"attr-table\"><tr><td class=\"attr-name\">.type_name</td><td class = \"attrvalue\">Examples</td></tr><tr><td class=\"attr-name\">._artifacts</td><td class = \"attrvalue\"><table class=\"attr-table\"><tr><td class=\"attr-name\">[0]</td><td class = \"attrvalue\"><style>\n",".tfx-object.expanded {\n","  padding: 4px 8px 4px 8px;\n","  background: white;\n","  border: 1px solid #bbbbbb;\n","  box-shadow: 4px 4px 2px rgba(0,0,0,0.05);\n","}\n",".tfx-object, .tfx-object * {\n","  font-size: 11pt;\n","}\n",".tfx-object > .title {\n","  cursor: pointer;\n","}\n",".tfx-object .expansion-marker {\n","  color: #999999;\n","}\n",".tfx-object.expanded > .title > .expansion-marker:before {\n","  content: '▼';\n","}\n",".tfx-object.collapsed > .title > .expansion-marker:before {\n","  content: '▶';\n","}\n",".tfx-object .class-name {\n","  font-weight: bold;\n","}\n",".tfx-object .deemphasize {\n","  opacity: 0.5;\n","}\n",".tfx-object.collapsed > table.attr-table {\n","  display: none;\n","}\n",".tfx-object.expanded > table.attr-table {\n","  display: block;\n","}\n",".tfx-object table.attr-table {\n","  border: 2px solid white;\n","  margin-top: 5px;\n","}\n",".tfx-object table.attr-table td.attr-name {\n","  vertical-align: top;\n","  font-weight: bold;\n","}\n",".tfx-object table.attr-table td.attrvalue {\n","  text-align: left;\n","}\n","</style>\n","<script>\n","function toggleTfxObject(element) {\n","  var objElement = element.parentElement;\n","  if (objElement.classList.contains('collapsed')) {\n","    objElement.classList.remove('collapsed');\n","    objElement.classList.add('expanded');\n","  } else {\n","    objElement.classList.add('collapsed');\n","    objElement.classList.remove('expanded');\n","  }\n","}\n","</script>\n","<div class=\"tfx-object collapsed\"><div class = \"title\" onclick=\"toggleTfxObject(this)\"><span class=\"expansion-marker\"></span><span class=\"class-name\">Artifact</span> of type <span class=\"class-name\">'Examples'</span> (uri: /content/tfx/CsvExampleGen/examples/1)<span class=\"deemphasize\"> at 0x7fc951fc4f10</span></div><table class=\"attr-table\"><tr><td class=\"attr-name\">.type</td><td class = \"attrvalue\">&lt;class &#x27;tfx.types.standard_artifacts.Examples&#x27;&gt;</td></tr><tr><td class=\"attr-name\">.uri</td><td class = \"attrvalue\">/content/tfx/CsvExampleGen/examples/1</td></tr><tr><td class=\"attr-name\">.span</td><td class = \"attrvalue\">0</td></tr><tr><td class=\"attr-name\">.split_names</td><td class = \"attrvalue\">[&quot;train&quot;, &quot;eval&quot;]</td></tr><tr><td class=\"attr-name\">.version</td><td class = \"attrvalue\">0</td></tr></table></div></td></tr></table></td></tr></table></div></td></tr></table></td></tr><tr><td class=\"attr-name\">.exec_properties</td><td class = \"attrvalue\"><table class=\"attr-table\"><tr><td class=\"attr-name\">['input_base']</td><td class = \"attrvalue\">/content/tfx/data</td></tr><tr><td class=\"attr-name\">['input_config']</td><td class = \"attrvalue\">{\n","  &quot;splits&quot;: [\n","    {\n","      &quot;name&quot;: &quot;single_split&quot;,\n","      &quot;pattern&quot;: &quot;*&quot;\n","    }\n","  ]\n","}</td></tr><tr><td class=\"attr-name\">['output_config']</td><td class = \"attrvalue\">{\n","  &quot;split_config&quot;: {\n","    &quot;splits&quot;: [\n","      {\n","        &quot;hash_buckets&quot;: 2,\n","        &quot;name&quot;: &quot;train&quot;\n","      },\n","      {\n","        &quot;hash_buckets&quot;: 1,\n","        &quot;name&quot;: &quot;eval&quot;\n","      }\n","    ]\n","  }\n","}</td></tr><tr><td class=\"attr-name\">['output_data_format']</td><td class = \"attrvalue\">6</td></tr><tr><td class=\"attr-name\">['custom_config']</td><td class = \"attrvalue\">None</td></tr><tr><td class=\"attr-name\">['range_config']</td><td class = \"attrvalue\">None</td></tr><tr><td class=\"attr-name\">['span']</td><td class = \"attrvalue\">0</td></tr><tr><td class=\"attr-name\">['version']</td><td class = \"attrvalue\">None</td></tr><tr><td class=\"attr-name\">['input_fingerprint']</td><td class = \"attrvalue\">split:single_split,num_files:1,total_bytes:907007,xor_checksum:1619341126,sum_checksum:1619341126</td></tr></table></td></tr></table></div></td></tr><tr><td class=\"attr-name\">.component.inputs</td><td class = \"attrvalue\">{}</td></tr><tr><td class=\"attr-name\">.component.outputs</td><td class = \"attrvalue\"><table class=\"attr-table\"><tr><td class=\"attr-name\">['examples']</td><td class = \"attrvalue\"><style>\n",".tfx-object.expanded {\n","  padding: 4px 8px 4px 8px;\n","  background: white;\n","  border: 1px solid #bbbbbb;\n","  box-shadow: 4px 4px 2px rgba(0,0,0,0.05);\n","}\n",".tfx-object, .tfx-object * {\n","  font-size: 11pt;\n","}\n",".tfx-object > .title {\n","  cursor: pointer;\n","}\n",".tfx-object .expansion-marker {\n","  color: #999999;\n","}\n",".tfx-object.expanded > .title > .expansion-marker:before {\n","  content: '▼';\n","}\n",".tfx-object.collapsed > .title > .expansion-marker:before {\n","  content: '▶';\n","}\n",".tfx-object .class-name {\n","  font-weight: bold;\n","}\n",".tfx-object .deemphasize {\n","  opacity: 0.5;\n","}\n",".tfx-object.collapsed > table.attr-table {\n","  display: none;\n","}\n",".tfx-object.expanded > table.attr-table {\n","  display: block;\n","}\n",".tfx-object table.attr-table {\n","  border: 2px solid white;\n","  margin-top: 5px;\n","}\n",".tfx-object table.attr-table td.attr-name {\n","  vertical-align: top;\n","  font-weight: bold;\n","}\n",".tfx-object table.attr-table td.attrvalue {\n","  text-align: left;\n","}\n","</style>\n","<script>\n","function toggleTfxObject(element) {\n","  var objElement = element.parentElement;\n","  if (objElement.classList.contains('collapsed')) {\n","    objElement.classList.remove('collapsed');\n","    objElement.classList.add('expanded');\n","  } else {\n","    objElement.classList.add('collapsed');\n","    objElement.classList.remove('expanded');\n","  }\n","}\n","</script>\n","<div class=\"tfx-object collapsed\"><div class = \"title\" onclick=\"toggleTfxObject(this)\"><span class=\"expansion-marker\"></span><span class=\"class-name\">Channel</span> of type <span class=\"class-name\">'Examples'</span> (1 artifact)<span class=\"deemphasize\"> at 0x7fc951fb5a10</span></div><table class=\"attr-table\"><tr><td class=\"attr-name\">.type_name</td><td class = \"attrvalue\">Examples</td></tr><tr><td class=\"attr-name\">._artifacts</td><td class = \"attrvalue\"><table class=\"attr-table\"><tr><td class=\"attr-name\">[0]</td><td class = \"attrvalue\"><style>\n",".tfx-object.expanded {\n","  padding: 4px 8px 4px 8px;\n","  background: white;\n","  border: 1px solid #bbbbbb;\n","  box-shadow: 4px 4px 2px rgba(0,0,0,0.05);\n","}\n",".tfx-object, .tfx-object * {\n","  font-size: 11pt;\n","}\n",".tfx-object > .title {\n","  cursor: pointer;\n","}\n",".tfx-object .expansion-marker {\n","  color: #999999;\n","}\n",".tfx-object.expanded > .title > .expansion-marker:before {\n","  content: '▼';\n","}\n",".tfx-object.collapsed > .title > .expansion-marker:before {\n","  content: '▶';\n","}\n",".tfx-object .class-name {\n","  font-weight: bold;\n","}\n",".tfx-object .deemphasize {\n","  opacity: 0.5;\n","}\n",".tfx-object.collapsed > table.attr-table {\n","  display: none;\n","}\n",".tfx-object.expanded > table.attr-table {\n","  display: block;\n","}\n",".tfx-object table.attr-table {\n","  border: 2px solid white;\n","  margin-top: 5px;\n","}\n",".tfx-object table.attr-table td.attr-name {\n","  vertical-align: top;\n","  font-weight: bold;\n","}\n",".tfx-object table.attr-table td.attrvalue {\n","  text-align: left;\n","}\n","</style>\n","<script>\n","function toggleTfxObject(element) {\n","  var objElement = element.parentElement;\n","  if (objElement.classList.contains('collapsed')) {\n","    objElement.classList.remove('collapsed');\n","    objElement.classList.add('expanded');\n","  } else {\n","    objElement.classList.add('collapsed');\n","    objElement.classList.remove('expanded');\n","  }\n","}\n","</script>\n","<div class=\"tfx-object collapsed\"><div class = \"title\" onclick=\"toggleTfxObject(this)\"><span class=\"expansion-marker\"></span><span class=\"class-name\">Artifact</span> of type <span class=\"class-name\">'Examples'</span> (uri: /content/tfx/CsvExampleGen/examples/1)<span class=\"deemphasize\"> at 0x7fc951fc4f10</span></div><table class=\"attr-table\"><tr><td class=\"attr-name\">.type</td><td class = \"attrvalue\">&lt;class &#x27;tfx.types.standard_artifacts.Examples&#x27;&gt;</td></tr><tr><td class=\"attr-name\">.uri</td><td class = \"attrvalue\">/content/tfx/CsvExampleGen/examples/1</td></tr><tr><td class=\"attr-name\">.span</td><td class = \"attrvalue\">0</td></tr><tr><td class=\"attr-name\">.split_names</td><td class = \"attrvalue\">[&quot;train&quot;, &quot;eval&quot;]</td></tr><tr><td class=\"attr-name\">.version</td><td class = \"attrvalue\">0</td></tr></table></div></td></tr></table></td></tr></table></div></td></tr></table></td></tr></table></div>"],"text/plain":["ExecutionResult(\n","    component_id: CsvExampleGen\n","    execution_id: 1\n","    outputs:\n","        examples: Channel(\n","            type_name: Examples\n","            artifacts: [Artifact(artifact: id: 1\n","        type_id: 5\n","        uri: \"/content/tfx/CsvExampleGen/examples/1\"\n","        properties {\n","          key: \"split_names\"\n","          value {\n","            string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n","          }\n","        }\n","        custom_properties {\n","          key: \"input_fingerprint\"\n","          value {\n","            string_value: \"split:single_split,num_files:1,total_bytes:907007,xor_checksum:1619341126,sum_checksum:1619341126\"\n","          }\n","        }\n","        custom_properties {\n","          key: \"payload_format\"\n","          value {\n","            string_value: \"FORMAT_TF_EXAMPLE\"\n","          }\n","        }\n","        custom_properties {\n","          key: \"span\"\n","          value {\n","            string_value: \"0\"\n","          }\n","        }\n","        custom_properties {\n","          key: \"state\"\n","          value {\n","            string_value: \"published\"\n","          }\n","        }\n","        custom_properties {\n","          key: \"tfx_version\"\n","          value {\n","            string_value: \"0.29.0\"\n","          }\n","        }\n","        state: LIVE\n","        , artifact_type: id: 5\n","        name: \"Examples\"\n","        properties {\n","          key: \"span\"\n","          value: INT\n","        }\n","        properties {\n","          key: \"split_names\"\n","          value: STRING\n","        }\n","        properties {\n","          key: \"version\"\n","          value: INT\n","        }\n","        )]\n","            additional_properties: {}\n","            additional_custom_properties: {}\n","        ))"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"OqCoZh7KPUm9"},"source":["Let's examine the output artifacts of `ExampleGen`. This component produces two artifacts, training examples and evaluation examples:"]},{"cell_type":"code","metadata":{"id":"880KkTAkPeUg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619341131859,"user_tz":420,"elapsed":15300,"user":{"displayName":"Riddhi Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggz1Tm3h5UIJzG3vpJXI5xI7vVZY0idHw_n7Lf7=s64","userId":"07907048644654273054"}},"outputId":"418c7794-9d05-441d-e7d8-d158c2d934db"},"source":["artifact = example_gen.outputs['examples'].get()[0]\n","print(artifact.split_names, artifact.uri)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[\"train\", \"eval\"] /content/tfx/CsvExampleGen/examples/1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"J6vcbW_wPqvl"},"source":["We can also take a look at the first three training examples:"]},{"cell_type":"code","metadata":{"id":"kOOKqSweWy9a"},"source":["# Get the URI of the output artifact representing the training examples, which is a directory\n","train_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'train')\n","\n","# Get the list of files in this directory (all compressed TFRecord files)\n","tfrecord_filenames = [os.path.join(train_uri, name)\n","                      for name in os.listdir(train_uri)]\n","\n","# Create a `TFRecordDataset` to read these files\n","dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n","\n","# Iterate over the first 3 records and decode them.\n","for tfrecord in dataset.take(3):\n","  serialized_example = tfrecord.numpy()\n","  example = tf.train.Example()\n","  example.ParseFromString(serialized_example)\n","  pp.pprint(example)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2gluYjccf-IP"},"source":["Now that `ExampleGen` has finished ingesting the data, the next step is data analysis."]},{"cell_type":"markdown","metadata":{"id":"csM6BFhtk5Aa"},"source":["### StatisticsGen\n","The `StatisticsGen` component **computes statistics** over your dataset for data analysis, as well as for use in downstream components. It uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library.\n","\n","`StatisticsGen` takes as input the dataset we just ingested using `ExampleGen`."]},{"cell_type":"code","metadata":{"id":"MAscCCYWgA-9"},"source":["statistics_gen = StatisticsGen(\n","    examples=example_gen.outputs['examples'])\n","context.run(statistics_gen)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HLI6cb_5WugZ"},"source":["After `StatisticsGen` finishes running, we can visualize the outputted statistics - **TFDV**. Try playing with the different plots!"]},{"cell_type":"code","metadata":{"id":"tLjXy7K6Tp_G"},"source":["context.show(statistics_gen.outputs['statistics'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HLKLTO9Nk60p"},"source":["### SchemaGen\n","\n","The `SchemaGen` component generates a schema based on your data statistics( outputs of StatisticsGen ). (A schema defines the expected bounds, types, and properties of the features in your dataset.) It also uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library.\n","\n","Note: The generated schema is best-effort and only tries to infer basic properties of the data. It is expected that you review and modify it as needed.\n","\n","`SchemaGen` will take as input the statistics that we generated with `StatisticsGen`, looking at the training split by default."]},{"cell_type":"code","metadata":{"id":"ygQvZ6hsiQ_J"},"source":["schema_gen = SchemaGen(\n","    statistics=statistics_gen.outputs['statistics'],\n","    infer_feature_shape=False)\n","context.run(schema_gen)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zi6TxTUKXM6b"},"source":["After `SchemaGen` finishes running, we can visualize the generated schema as a table."]},{"cell_type":"code","metadata":{"id":"Ec9vqDXpXeMb"},"source":["context.show(schema_gen.outputs['schema'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kZWWdbA-m7zp"},"source":["Each feature in your dataset shows up as a row in the schema table, alongside its properties. The schema also captures all the values that a categorical feature takes on, denoted as its domain.\n","\n","To learn more about schemas, see [the SchemaGen documentation](https://www.tensorflow.org/tfx/guide/schemagen)."]},{"cell_type":"markdown","metadata":{"id":"V1qcUuO9k9f8"},"source":["### ExampleValidator\n","The `ExampleValidator` component detects anomalies in your data, based on the expectations defined by the schema. It also uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library.\n","\n","`ExampleValidator` will take as input the statistics from `StatisticsGen`, and the schema from `SchemaGen`."]},{"cell_type":"code","metadata":{"id":"XRlRUuGgiXks"},"source":["example_validator = ExampleValidator(\n","    statistics=statistics_gen.outputs['statistics'],\n","    schema=schema_gen.outputs['schema'])\n","context.run(example_validator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"855mrHgJcoer"},"source":["After `ExampleValidator` finishes running, we can visualize the anomalies as a table."]},{"cell_type":"code","metadata":{"id":"TDyAAozQcrk3"},"source":["context.show(example_validator.outputs['anomalies'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"znMoJj60ybZx"},"source":["In the anomalies table, we can see that there are no anomalies. This is what we'd expect, since this the first dataset that we've analyzed and the schema is tailored to it. You should review this schema -- anything unexpected means an anomaly in the data. Once reviewed, the schema can be used to guard future data, and anomalies produced here can be used to debug model performance, understand how your data evolves over time, and identify data errors."]},{"cell_type":"code","metadata":{"id":"pwbW2zPKR_S4"},"source":["# Get the URI of the output artifact representing the transformed examples, which is a directory\n","train_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'train')\n","\n","# Get the list of files in this directory (all compressed TFRecord files)\n","tfrecord_filenames = [os.path.join(train_uri, name)\n","                      for name in os.listdir(train_uri)]\n","\n","# Create a `TFRecordDataset` to read these files\n","dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n","\n","# Iterate over the first 1 records and decode them.\n","for tfrecord in dataset.take(1):\n","  serialized_example = tfrecord.numpy()\n","  example = tf.train.Example()\n","  example.ParseFromString(serialized_example)\n","  pp.pprint(example)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JtyRz53ZGNDH"},"source":["### Transform\n","We can use TFT here but, specifically I am using other options like constants calculated via Pandas / Numpy etc. These all will be stored on a **constants_trainer.py** file and then used in trainer."]},{"cell_type":"code","metadata":{"id":"GPsP28Zyr3pk"},"source":["bins_lat = pd.qcut(list(df['dropoff_latitude'].values) + list(df['pickup_latitude'].values), q=20, duplicates='drop', retbins=True)[1]\n","bins_lon = pd.qcut(list(df['dropoff_longitude'].values) + list(df['pickup_longitude'].values), q=20, duplicates='drop', retbins=True)[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T-zikEmDGkpf"},"source":["code = '''\n","bins_lat = {bins_lat}\n","bins_lon = {bins_lon}\n","'''\n","\n","code = code.replace('{bins_lat}', str(list(bins_lat)))\n","code = code.replace('{bins_lon}', str(list(bins_lon)))\n","\n","with open(_constants_module_file, 'w') as writefile:\n","    writefile.write(code)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q_b_V6eN4f69"},"source":["After the `Transform` component has transformed your data into features, and the next step is to train a model."]},{"cell_type":"markdown","metadata":{"id":"OBJFtnl6lCg9"},"source":["### Trainer\n","The `Trainer` component will train a model that you define in TensorFlow. Default Trainer support Estimator API, to use Keras API, you need to specify [Generic Trainer](https://github.com/tensorflow/community/blob/master/rfcs/20200117-tfx-generic-trainer.md) by setup `custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor)` in Trainer's contructor.\n","\n","`Trainer` takes as input the schema from `SchemaGen`, the transformed data and graph from `Transform`, training parameters, as well as a module that contains user-defined model code.\n","\n","Will generate two files: \n","- **inputfn_trainer.py** *Data-Feeder to model\n","- **model_trainer.py** *Trainer module"]},{"cell_type":"code","metadata":{"id":"R0kTyFLBCMNI"},"source":["%%writefile {_input_fn_module_file}\n","\n","import os\n","import tensorflow as tf\n","\n","###############################\n","##Feature engineering functions\n","def feature_engg_features(features):\n","  #Add new features\n","  features['distance'] = ((features['pickup_latitude'] - features['dropoff_latitude'])**2 +  (features['pickup_longitude'] - features['dropoff_longitude'])**2)**0.5\n","  features['trip_start_month'] = tf.strings.as_string(features['trip_start_month'])\n","  features['trip_start_hour'] = tf.strings.as_string(features['trip_start_hour'])\n","  features['trip_start_day'] = tf.strings.as_string(features['trip_start_day'])\n","\n","  return(features)\n","\n","#To be called from TF\n","def feature_engg(features, label):\n","  #Add new features\n","  features = feature_engg_features(features)\n","\n","  return(features, label)\n","\n","def make_input_fn(dir_uri, mode, vnum_epochs = None, batch_size = 512):\n","    def decode_tfr(serialized_example):\n","      # 1. define a parser\n","      features = tf.io.parse_example(\n","        serialized_example,\n","        # Defaults are not specified since both keys are required.\n","        features={\n","            'dropoff_latitude': tf.io.FixedLenFeature([], tf.float32),\n","            'dropoff_longitude': tf.io.FixedLenFeature([], tf.float32),\n","            'fare': tf.io.FixedLenFeature([], tf.float32),\n","            'pickup_latitude': tf.io.FixedLenFeature([], tf.float32, default_value = 0.0),\n","            'pickup_longitude': tf.io.FixedLenFeature([], tf.float32, default_value = 0.0),\n","            'trip_start_day': tf.io.FixedLenFeature([], tf.int64),\n","            'trip_start_hour': tf.io.FixedLenFeature([], tf.int64),\n","            'trip_start_month': tf.io.FixedLenFeature([], tf.int64)\n","        })\n","\n","      return features, features['fare']\n","\n","    def _input_fn(v_test=False):\n","      # Get the list of files in this directory (all compressed TFRecord files)\n","      tfrecord_filenames = tf.io.gfile.glob(dir_uri)\n","\n","      # Create a `TFRecordDataset` to read these files\n","      dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n","\n","      if mode == tf.estimator.ModeKeys.TRAIN:\n","        num_epochs = vnum_epochs # indefinitely\n","      else:\n","        num_epochs = 1 # end-of-input after this\n","\n","      dataset = dataset.batch(batch_size)\n","      dataset = dataset.prefetch(buffer_size = batch_size)\n","\n","      #Convert TFRecord data to dict\n","      dataset = dataset.map(decode_tfr)\n","\n","      #Feature engineering\n","      dataset = dataset.map(feature_engg)\n","\n","      if mode == tf.estimator.ModeKeys.TRAIN:\n","          num_epochs = vnum_epochs # indefinitely\n","          dataset = dataset.shuffle(buffer_size = batch_size)\n","      else:\n","          num_epochs = 1 # end-of-input after this\n","\n","      dataset = dataset.repeat(num_epochs)       \n","      \n","      #Begins - Uncomment for testing only -----------------------------------------------------<\n","      if v_test == True:\n","        print(next(dataset.__iter__()))\n","        \n","      #End - Uncomment for testing only -----------------------------------------------------<\n","      return dataset\n","    return _input_fn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z6QlsOPtDCQE"},"source":["##Test the input function\n","import inputfn_trainer as ift\n","\n","#Test dataset read + Feat Engg function's - output's CSV + Feature engg columns\n","eval_file = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'eval/*')\n","fn_d = ift.make_input_fn(dir_uri = eval_file,\n","                    mode = tf.estimator.ModeKeys.EVAL,\n","                    # vnum_epochs = 1,\n","                    batch_size = 10)\n","\n","fn_d(v_test=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nf9UuNng4YJu"},"source":["%%writefile {_model_trainer_module_file}\n","\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","import inputfn_trainer as ift\n","import constants_trainer as ct\n","\n","from tfx.components.trainer.fn_args_utils import FnArgs\n","print(tf.__version__)\n","\n","device = \"gpu\"\n","\n","if device == \"tpu\":\n","  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n","  tf.config.experimental_connect_to_cluster(resolver)\n","  # This is the TPU initialization code that has to be at the beginning.\n","  tf.tpu.experimental.initialize_tpu_system(resolver)\n","  strategy = tf.distribute.experimental.TPUStrategy(resolver)\n","else:\n","  strategy = tf.distribute.MultiWorkerMirroredStrategy()\n","\n","#Create model\n","params_default = {\n","    'lr' : 0.001,\n","    'beta_1' : 0.99,\n","    'beta_2' : 0.999,\n","    'epsilon' : 1e-08,\n","    'decay' : 0.01,\n","    'hidden_layers' : 1\n","}\n","\n","# Define feature columns(Including feature engineered ones )\n","# These are the features which come from the TF Data pipeline\n","def create_feature_cols():\n","    #Keras format features\n","    k_month = tf.keras.Input(name='trip_start_month', shape=(1,), dtype=tf.string)\n","    k_hour  = tf.keras.Input(name='trip_start_hour', shape=(1,), dtype=tf.string)\n","    k_day  = tf.keras.Input(name='trip_start_day', shape=(1,), dtype=tf.string)\n","    k_picklat  = tf.keras.Input(name='pickup_latitude', shape=(1,), dtype=tf.float32)\n","    k_picklon  = tf.keras.Input(name='pickup_longitude', shape=(1,), dtype=tf.float32)\n","    k_droplat  = tf.keras.Input(name='dropoff_latitude', shape=(1,), dtype=tf.float32)\n","    k_droplon  = tf.keras.Input(name='dropoff_longitude', shape=(1,), dtype=tf.float32)\n","    k_distance  = tf.keras.Input(name='distance', shape=(1,), dtype=tf.float32)\n","    keras_dict_input = {'trip_start_month': k_month, 'trip_start_hour': k_hour, 'trip_start_day' : k_day,\n","                        'pickup_latitude': k_picklat, 'pickup_longitude': k_picklon,\n","                        'dropoff_latitude': k_droplat, 'dropoff_longitude': k_droplon, 'distance' : k_distance\n","                        }\n","\n","    return({'K' : keras_dict_input})\n","\n","def create_keras_model(feature_cols, bins_lat, bins_lon,  params = params_default):\n","    METRICS = [\n","            keras.metrics.RootMeanSquaredError(name='rmse')\n","    ]\n","\n","    #Input layers\n","    input_feats = []\n","    for inp in feature_cols['K'].keys():\n","      input_feats.append(feature_cols['K'][inp])\n","\n","    ##Input processing\n","    ##https://keras.io/examples/structured_data/structured_data_classification_from_scratch/\n","    ##https://github.com/tensorflow/community/blob/master/rfcs/20191212-keras-categorical-inputs.md\n","\n","    ##Handle categorical attributes( One-hot encoding )\n","    cat_day = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=['1','2','3','4','5','6','7'], mask_token=None)(feature_cols['K']['trip_start_day'])\n","    cat_day = tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=7)(cat_day)\n","\n","    cat_hour = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=['1','2','3','4','5','6','7','8'\n","                                                                                      '9','10','11','12','13','14','15','16',\n","                                                                                      '17','18','19','20','21','22','23','0'\n","                                                                                      ], mask_token=None)(feature_cols['K']['trip_start_hour'])\n","    cat_hour = tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=24)(cat_hour)\n","\n","    cat_month = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=['1','2','3','4','5','6','7','8'\n","                                                                                      '9','10','11','12'], mask_token=None)(feature_cols['K']['trip_start_month'])\n","    cat_month = tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=12)(cat_month)\n","\n","    # cat_company = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=df['company'].unique(), mask_token=None)(feature_cols['K']['company'])\n","    # cat_company = tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=len(df['company'].unique()))(cat_company)\n","\n","    ##Binning\n","    bins_pickup_lat = tf.keras.layers.experimental.preprocessing.Discretization(bins = bins_lat)(feature_cols['K']['pickup_latitude'])\n","    cat_pickup_lat = tf.keras.layers.experimental.preprocessing.CategoryEncoding(len(bins_lat)+1)(bins_pickup_lat)\n","\n","    bins_pickup_lon = tf.keras.layers.experimental.preprocessing.Discretization(bins = bins_lon)(feature_cols['K']['pickup_longitude'])\n","    cat_pickup_lon = tf.keras.layers.experimental.preprocessing.CategoryEncoding(len(bins_lon)+1)(bins_pickup_lon)\n","\n","    bins_drop_lat = tf.keras.layers.experimental.preprocessing.Discretization(bins = bins_lat)(feature_cols['K']['dropoff_latitude'])\n","    cat_drop_lat = tf.keras.layers.experimental.preprocessing.CategoryEncoding(len(bins_lat)+1)(bins_drop_lat)\n","\n","    bins_drop_lon = tf.keras.layers.experimental.preprocessing.Discretization(bins = bins_lon)(feature_cols['K']['dropoff_longitude'])\n","    cat_drop_lon = tf.keras.layers.experimental.preprocessing.CategoryEncoding(len(bins_lon)+1)(bins_drop_lon)\n","\n","    ##Categorical cross\n","    cross_day_hour = tf.keras.layers.experimental.preprocessing.CategoryCrossing()([cat_day, cat_hour])\n","    hash_cross_day_hour = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=24 * 7)(cross_day_hour)\n","    cat_cross_day_hour = tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens = 24* 7)(hash_cross_day_hour)\n","\n","    cross_pick_lon_lat = tf.keras.layers.experimental.preprocessing.CategoryCrossing()([cat_pickup_lat, cat_pickup_lon])\n","    hash_cross_pick_lon_lat = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=(len(bins_lat) + 1) ** 2)(cross_pick_lon_lat)\n","\n","    cross_drop_lon_lat = tf.keras.layers.experimental.preprocessing.CategoryCrossing()([cat_drop_lat, cat_drop_lon])\n","    hash_cross_drop_lon_lat = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=(len(bins_lat) + 1) ** 2)(cross_drop_lon_lat)\n","\n","    # Cross to embedding\n","    embed_cross_pick_lon_lat = tf.keras.layers.Embedding(((len(bins_lat) + 1) ** 2), 4)(hash_cross_pick_lon_lat)\n","    embed_cross_pick_lon_lat = tf.reduce_sum(embed_cross_pick_lon_lat, axis=-2)\n","\n","    embed_cross_drop_lon_lat = tf.keras.layers.Embedding(((len(bins_lat) + 1) ** 2), 4)(hash_cross_drop_lon_lat)\n","    embed_cross_drop_lon_lat = tf.reduce_sum(embed_cross_drop_lon_lat, axis=-2)\n","\n","    # Also pass time attributes as Deep signal( Cast to integer )\n","    int_trip_start_day = tf.strings.to_number(feature_cols['K']['trip_start_day'], tf.float32)\n","    int_trip_start_hour = tf.strings.to_number(feature_cols['K']['trip_start_hour'], tf.float32)\n","    int_trip_start_month = tf.strings.to_number(feature_cols['K']['trip_start_month'], tf.float32)\n","\n","    #Add feature engineered columns - LAMBDA layer\n","\n","    ###Create MODEL\n","    ####Concatenate all features( Numerical input )\n","    x_input_numeric = tf.keras.layers.concatenate([\n","                    feature_cols['K']['pickup_latitude'], feature_cols['K']['pickup_longitude'],\n","                    feature_cols['K']['dropoff_latitude'], feature_cols['K']['dropoff_longitude'],\n","                    feature_cols['K']['distance'], embed_cross_pick_lon_lat, embed_cross_drop_lon_lat,\n","                    int_trip_start_day, int_trip_start_hour, int_trip_start_month\n","                    ])\n","\n","    #DEEP - This Dense layer connects to input layer - Numeric Data\n","    x_numeric = tf.keras.layers.Dense(32, activation='relu', kernel_initializer=\"he_uniform\")(x_input_numeric)\n","    x_numeric = tf.keras.layers.BatchNormalization()(x_numeric)\n","\n","    ####Concatenate all Categorical features( Categorical converted )\n","    x_input_categ = tf.keras.layers.concatenate([\n","                    cat_month, cat_cross_day_hour, cat_pickup_lat, cat_pickup_lon,\n","                    cat_drop_lat, cat_drop_lon\n","                    ])\n","    \n","    #WIDE - This Dense layer connects to input layer - Categorical Data\n","    x_categ = tf.keras.layers.Dense(32, activation='relu', kernel_initializer=\"he_uniform\")(x_input_categ)\n","\n","    ####Concatenate both Wide and Deep layers\n","    x = tf.keras.layers.concatenate([x_categ, x_numeric])\n","\n","    for l_ in range(params['hidden_layers']):\n","        x = tf.keras.layers.Dense(32, activation='relu', kernel_initializer=\"he_uniform\",\n","                                  activity_regularizer=tf.keras.regularizers.l2(0.00001))(x)\n","        x = tf.keras.layers.BatchNormalization()(x)\n","\n","    #Final Layer\n","    out = tf.keras.layers.Dense(1, activation='relu')(x)\n","    model = tf.keras.Model(input_feats, out)\n","\n","    #Set optimizer\n","    opt = tf.keras.optimizers.Adam(lr= params['lr'], beta_1=params['beta_1'], \n","                                        beta_2=params['beta_2'], epsilon=params['epsilon'])\n","\n","    #Compile model\n","    model.compile(loss='mean_squared_error',  optimizer=opt, metrics = METRICS)\n","\n","    #Print Summary\n","    print(model.summary())\n","    return model\n","\n","def keras_train_and_evaluate(model, train_dataset, validation_dataset, epochs=100):\n","  #Add callbacks\n","  reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n","                                patience=5, min_lr=0.00001, verbose = 1)\n","  \n","  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n","\n","  #Train and Evaluate\n","  out = model.fit(train_dataset, \n","                  validation_data = validation_dataset,\n","                  epochs=epochs,\n","                  # validation_steps = 3,   ###Keep this none for running evaluation on full EVAL data every epoch\n","                  steps_per_epoch = 100,   ###Has to be passed - Cant help it :) [ Number of batches per epoch ]\n","                  callbacks=[reduce_lr, #modelsave_callback, #tensorboard_callback, \n","                             keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True, verbose=True)]\n","                  )\n","\n","  return model\n","\n","def save_model(model, model_save_path):\n","  @tf.function\n","  def serving(dropoff_latitude, dropoff_longitude, pickup_latitude, pickup_longitude, trip_start_day, trip_start_hour, trip_start_month):\n","      ##Feature engineering( calculate distance )\n","      distance = tf.cast( tf.sqrt((tf.abs(dropoff_latitude - pickup_latitude))**2 + (tf.abs(dropoff_longitude - pickup_longitude))**2), tf.float32)\n","\n","      payload = {\n","          'dropoff_latitude': dropoff_latitude,\n","          'dropoff_longitude': dropoff_longitude,\n","          'pickup_latitude': pickup_latitude,\n","          'pickup_longitude': pickup_longitude,\n","          'trip_start_day': trip_start_day,\n","          'trip_start_hour': trip_start_hour,\n","          'trip_start_month': trip_start_month,\n","          'distance': distance\n","      }\n","      \n","      ## Predict\n","      ##IF THERE IS AN ERROR IN NUMBER OF PARAMS PASSED HERE OR DATA TYPE THEN IT GIVES ERROR, \"COULDN'T COMPUTE OUTPUT TENSOR\"\n","      predictions = model(payload)\n","      return predictions\n","\n","  serving = serving.get_concrete_function(trip_start_day=tf.TensorSpec([None,], dtype= tf.string, name='trip_start_day'), \n","                                          trip_start_hour=tf.TensorSpec([None,], dtype= tf.string, name='trip_start_hour'),\n","                                          trip_start_month=tf.TensorSpec([None], dtype= tf.string, name='trip_start_month'), \n","                                          dropoff_latitude=tf.TensorSpec([None,], dtype= tf.float32, name='dropoff_latitude'),\n","                                          dropoff_longitude=tf.TensorSpec([None,], dtype= tf.float32, name='dropoff_longitude'), \n","                                          pickup_latitude=tf.TensorSpec([None,], dtype= tf.float32, name='pickup_latitude'),\n","                                          pickup_longitude=tf.TensorSpec([None,], dtype= tf.float32, name='pickup_longitude')\n","                                          )\n","\n","  # version = \"1\"  #{'serving_default': call_output}\n","  tf.saved_model.save(\n","      model,\n","      model_save_path + \"/\",\n","      signatures=serving\n","  )\n","\n","##Main function called by TFX\n","def run_fn(fn_args: FnArgs):\n","  #Create dataset input functions\n","  train_dataset = ift.make_input_fn(dir_uri = fn_args.train_files,\n","                      mode = tf.estimator.ModeKeys.TRAIN,\n","                      batch_size = 128)()\n","\n","  validation_dataset = ift.make_input_fn(dir_uri = fn_args.eval_files,\n","                      mode = tf.estimator.ModeKeys.EVAL,\n","                      batch_size = 512)()\n","\n","  #Create model\n","  m_ = create_keras_model(params = params_default, feature_cols = create_feature_cols(),\n","                          bins_lat = ct.bins_lat,\n","                          bins_lon = ct.bins_lon)\n","  tf.keras.utils.plot_model(m_, show_shapes=True, rankdir=\"LR\")\n","\n","  #Train model\n","  m_ = keras_train_and_evaluate(m_, train_dataset, validation_dataset, fn_args.custom_config['epochs'])\n","\n","  #Save model with custom signature\n","  save_model(m_, fn_args.serving_model_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GY4yTRaX4YJx"},"source":["Now, we pass in this model code to the `Trainer` component and run it to train the model."]},{"cell_type":"code","metadata":{"id":"429-vvCWibO0"},"source":["trainer = Trainer(\n","    module_file=os.path.abspath(_model_trainer_module_file),\n","    custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n","    examples=example_gen.outputs['examples'],\n","    train_args=trainer_pb2.TrainArgs(),\n","    eval_args=trainer_pb2.EvalArgs(),\n","    custom_config=({\"epochs\": 1})\n","    )\n","\n","context.run(trainer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Cql1G35StJp"},"source":["#### Analyze Training with TensorBoard\n","Take a peek at the trainer artifact. It points to a directory containing the model subdirectories."]},{"cell_type":"code","metadata":{"id":"bXe62WE0S0Ek"},"source":["model_artifact_dir = trainer.outputs['model'].get()[0].uri\n","pp.pprint(os.listdir(model_artifact_dir))\n","model_dir = os.path.join(model_artifact_dir, 'serving_model_dir')\n","pp.pprint(os.listdir(model_dir))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DfjOmSro6Q3Y"},"source":["Optionally, we can connect TensorBoard to the Trainer to analyze our model's training curves."]},{"cell_type":"code","metadata":{"id":"-APzqz2NeAyj"},"source":["# model_run_artifact_dir = trainer.outputs['model_run'].get()[0].uri\n","\n","# %load_ext tensorboard\n","# %tensorboard --logdir {model_run_artifact_dir}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T8DYekCZlHfj"},"source":["### Pusher\n","The `Pusher` component is usually at the end of a TFX pipeline. It checks whether a model has passed validation, and if so, exports the model to `_serving_model_dir`."]},{"cell_type":"code","metadata":{"id":"r45nQ69eikc9"},"source":["pusher = Pusher(\n","    model=trainer.outputs['model'],\n","    push_destination=pusher_pb2.PushDestination(\n","        filesystem=pusher_pb2.PushDestination.Filesystem(\n","            base_directory=_serving_model_dir)))\n","context.run(pusher)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ctUErBYoTO9I"},"source":["Let's examine the output artifacts of `Pusher`. "]},{"cell_type":"code","metadata":{"id":"pRkWo-MzTSss"},"source":["pusher.outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"peH2PPS3VgkL"},"source":["In particular, the Pusher will export your model in the SavedModel format, which looks like this:"]},{"cell_type":"code","metadata":{"id":"4zyIqWl9TSdG"},"source":["push_uri = pusher.outputs.pushed_model.get()[0].uri\n","model = tf.saved_model.load(push_uri)\n","\n","for item in model.signatures.items():\n","  pp.pprint(item)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZW7iMG0FkQJj"},"source":["###**Full Pipeline**\n","\n","The pipeline can be run on either of the below Orchestrators:\n","1. Local\n","2. Airflow\n","3. Kubeflow"]},{"cell_type":"code","metadata":{"id":"GAFjQv7hZ71T"},"source":["!rm -rf data.*\n","# !rm -rf *trainer.py ##EDIT: Python files have to be retained\n","!rm -rf *.csv\n","!sudo rm -r /content/tfx\n","\n","! cd /content/\n","! mkdir /content/tfx/\n","! mkdir /content/tfx/pipelines\n","! mkdir /content/tfx/metadata\n","! mkdir /content/tfx/logs\n","! mkdir /content/tfx/data\n","! mkdir /content/tfx/serving_model\n","\n","! mkdir /content/train_data/\n","! mkdir /content/eval_data/\n","\n","!wget https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RtsNVH2QaWRA"},"source":["df = pd.read_csv('/content/data.csv')\n","\n","##Drop useless columns\n","df = df.drop(['trip_start_timestamp','trip_miles','pickup_census_tract',\n","              'dropoff_census_tract','trip_seconds','payment_type','tips', \n","              'company','dropoff_community_area','pickup_community_area'], axis=1)\n","\n","#Drop NA rows\n","df = df.dropna()\n","\n","##Keep a test set for final testing( TFX internally splits train and validation data )\n","np.random.seed(seed=2)\n","msk = np.random.rand(len(df)) < 0.9\n","traindf = df[msk]\n","evaldf = df[~msk]\n","\n","print(len(traindf))\n","print(len(evaldf))\n","\n","traindf.to_csv(\"/content/train_data/data.csv\", index=False, header=True)\n","evaldf.to_csv(\"/content/eval_data/eval.csv\", index=False, header=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6RLIo8_LeB9k"},"source":["# https://github.com/tensorflow/tfx/blob/master/tfx/examples/chicago_taxi_pipeline/\n","def create_final_pipeline(\n","    pipeline_name: Text,\n","    root_path: Text,\n","    data_path: Text,\n","    training_params: Dict[Text, Text],\n","    # beam_pipeline_args: List[Text],\n",") -> pipeline.Pipeline:\n","\n","  _pipeline_root = os.path.join(root_path, 'pipelines');      # Join ~/tfx/pipelines/\n","  _metadata_db_root = os.path.join(root_path, 'metadata.db');    # Join ~/tfx/metadata.db\n","  _log_root = os.path.join(root_path, 'logs');\n","  _model_root = os.path.join(root_path, 'model');\n","  _serving_model_dir = os.path.join(root_path, 'serving_model')\n","\n","  # Full pipeline\n","  example_gen = CsvExampleGen(input=external_input(data_path))\n","\n","  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n","\n","  infer_schema = SchemaGen(\n","      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=False)\n","\n","  validate_stats = ExampleValidator(\n","    statistics=statistics_gen.outputs['statistics'],\n","    schema=infer_schema.outputs['schema'])\n","\n","  trainer = Trainer(\n","      module_file=os.path.abspath(_model_trainer_module_file),\n","      custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n","      examples=example_gen.outputs['examples'],\n","      train_args=trainer_pb2.TrainArgs(),\n","      eval_args=trainer_pb2.EvalArgs(),\n","      custom_config=(training_params)\n","      )\n","\n","  pusher = Pusher(\n","      model=trainer.outputs['model'],\n","      push_destination=pusher_pb2.PushDestination(\n","          filesystem=pusher_pb2.PushDestination.Filesystem(\n","              base_directory=_serving_model_dir)))\n","\n","  # This pipeline obj carries the business logic of the pipeline, but no runner-specific information\n","  # was included.\n","  return pipeline.Pipeline(\n","    pipeline_name=  pipeline_name,\n","    pipeline_root=  root_path,\n","    components=[\n","        example_gen, statistics_gen, infer_schema, validate_stats,\n","        trainer, pusher\n","    ],\n","    # metadata_connection_config = metadata.sqlite_metadata_connection_config(_metadata_db_root),\n","    metadata_connection_config = metadata.sqlite_metadata_connection_config(_metadata_db_root),\n","    enable_cache=True,\n","    beam_pipeline_args=['--direct_num_workers=%d' % 0],\n","  )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7cb_oBqYfjHr"},"source":["#Run pipeline locally\n","from tfx.orchestration.local.local_dag_runner import LocalDagRunner\n","\n","##Define all paths\n","_tfx_root = os.path.join(os.getcwd(), 'tfx')\n","\n","#Config params\n","training_params = {\"epochs\": 50}\n","\n","#Create and run pipeline\n","p_ = create_final_pipeline(root_path = _tfx_root, \n","                           pipeline_name=\"local_pipeline\", \n","                           data_path=\"/content/train_data\",\n","                           training_params=training_params)\n","\n","LocalDagRunner().run(p_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YjkNK4tjThRD"},"source":["### **Inference**( saved_model_cli )"]},{"cell_type":"code","metadata":{"id":"x1dvXxsiSSSJ"},"source":["!saved_model_cli show --dir \"/content/tfx/Pusher/pushed_model/4\" --all"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ws9TkuU5SpdU"},"source":["#LOCAL: Predict using Keras prediction function\n","saved_mod = tf.saved_model.load(\"/content/tfx/Pusher/pushed_model/4\")\n","\n","#Get prediction function from serving\n","f = saved_mod.signatures['serving_default']\n","\n","#Run prediction function from serving\n","f(dropoff_latitude=tf.convert_to_tensor([41.920452]), dropoff_longitude = tf.convert_to_tensor([-87.679955]), pickup_latitude = tf.convert_to_tensor([41.952823]), \n","  pickup_longitude =tf.convert_to_tensor([-87.653244]), trip_start_day=tf.convert_to_tensor([\"1\"]), trip_start_hour=tf.convert_to_tensor([\"5\"]),\n","  trip_start_month=tf.convert_to_tensor([\"6\"]))"],"execution_count":null,"outputs":[]}]}